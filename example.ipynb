{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def recall_cnn(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision_cnn(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1_cnn(y_true, y_pred):\n",
    "    precision = precision_cnn(y_true, y_pred)\n",
    "    recall = recall_cnn(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "METRICS = [\"accuracy\", recall_cnn, precision_cnn, f1_cnn]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def visualize_history(history):\n",
    "    #  \"Plot Accuracy\"\n",
    "    plt.plot(history.history[\"accuracy\"])\n",
    "    plt.plot(history.history[\"val_accuracy\"])\n",
    "    plt.title(\"model accuracy\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    # \"Plot Loss\"\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.plot(history.history[\"val_loss\"])\n",
    "    plt.title(\"model loss\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def Train_CNN_Model(epochs_num, model, train_iter, valid_iter, export_dir=\"./export\", name=\"default\"):\n",
    "    # -------------------------------------------------------------------------\n",
    "    #                        Train CNN Model\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    epochs = epochs_num\n",
    "\n",
    "    es = EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", verbose=1, patience=40)\n",
    "\n",
    "    mc = ModelCheckpoint(f\"{export_dir}/model_{name}.h5\", monitor=\"val_accuracy\", mode=\"max\", save_best_only=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_iter,\n",
    "        steps_per_epoch=len(train_iter),\n",
    "        validation_data=valid_iter,\n",
    "        validation_steps=len(valid_iter),\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        callbacks=[mc, es],\n",
    "    )\n",
    "\n",
    "    return history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " block1_conv1 (Conv2D)       (None, 256, 256, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 256, 256, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 128, 128, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 128, 128, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 128, 128, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 64, 64, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 64, 64, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 64, 64, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 64, 64, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 32, 32, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 32, 32, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 16, 16, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def VGG16(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            input_shape=input_shape,\n",
    "            name=\"block1_conv1\",\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "    )\n",
    "    model.add(BatchNormalization(name=\"bn1\"))\n",
    "\n",
    "    model.add(\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", name=\"block1_conv2\", activation=\"relu\")\n",
    "    )\n",
    "    model.add(BatchNormalization(name=\"bn2\"))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"same\", name=\"block1_pool\"))\n",
    "\n",
    "    model.add(\n",
    "        Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", name=\"block2_conv1\", activation=\"relu\")\n",
    "    )\n",
    "    model.add(BatchNormalization(name=\"bn3\"))\n",
    "\n",
    "    model.add(\n",
    "        Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", name=\"block2_conv2\", activation=\"relu\")\n",
    "    )\n",
    "    model.add(BatchNormalization(name=\"bn4\"))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"same\", name=\"block2_pool\"))\n",
    "\n",
    "    model.add(\n",
    "        Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", name=\"block3_conv1\", activation=\"relu\")\n",
    "    )\n",
    "    model.add(BatchNormalization(name=\"bn5\"))\n",
    "\n",
    "    model.add(\n",
    "        Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", name=\"block3_conv2\", activation=\"relu\")\n",
    "    )\n",
    "    model.add(BatchNormalization(name=\"bn6\"))\n",
    "\n",
    "    model.add(\n",
    "        Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", name=\"block3_conv3\", activation=\"relu\")\n",
    "    )\n",
    "    model.add(BatchNormalization(name=\"bn7\"))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"same\", name=\"block3_pool\"))\n",
    "    model.add(\n",
    "        Conv2D(filters=512, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", name=\"block4_conv1\", activation=\"relu\")\n",
    "    )\n",
    "    model.add(BatchNormalization(name=\"bn8\"))\n",
    "\n",
    "    model.add(\n",
    "        Conv2D(filters=512, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", name=\"block4_conv2\", activation=\"relu\")\n",
    "    )\n",
    "    model.add(BatchNormalization(name=\"bn9\"))\n",
    "\n",
    "    model.add(\n",
    "        Conv2D(filters=512, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", name=\"block4_conv3\", activation=\"relu\")\n",
    "    )\n",
    "    model.add(BatchNormalization(name=\"bn10\"))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"same\", name=\"block4_pool\"))\n",
    "    model.add(\n",
    "        Conv2D(filters=512, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", name=\"block5_conv1\", activation=\"relu\")\n",
    "    )\n",
    "    model.add(BatchNormalization(name=\"bn11\"))\n",
    "\n",
    "    model.add(\n",
    "        Conv2D(filters=512, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", name=\"block5_conv2\", activation=\"relu\")\n",
    "    )\n",
    "    model.add(BatchNormalization(name=\"bn12\"))\n",
    "\n",
    "    model.add(\n",
    "        Conv2D(filters=512, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", name=\"block5_conv3\", activation=\"relu\")\n",
    "    )\n",
    "    model.add(BatchNormalization(name=\"bn13\"))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"same\", name=\"block5_pool\"))\n",
    "\n",
    "    model.add(Flatten(name=\"flatten\"))\n",
    "    model.add(Dense(units=4096, activation=\"relu\", name=\"fc1\"))\n",
    "    model.add(Dense(units=4096, activation=\"relu\", name=\"fc2\"))\n",
    "    model.add(Dense(units=num_classes, activation=\"softmax\", name=\"predictions\"))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_shape = 224, 224, 3\n",
    "model_VGG16 = VGG16(input_shape, 8)\n",
    "model_VGG16.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3693 images belonging to 8 classes.\n",
      "Found 928 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "data_folder = 'data/Flowers'\n",
    "train_data_folder = 'data/train'\n",
    "validation_data_folder = 'data/validation'\n",
    "flower_types = [\"Babi\", \"Calimerio\", \"Chrysanthemum\", \"Hydrangeas\", \"Lisianthus\", \"Pingpong\", \"Rosy\", \"Tana\"]\n",
    "\n",
    "# Create empty lists to store image paths and corresponding labels\n",
    "train_image_paths = []\n",
    "train_labels = []\n",
    "val_image_paths = []\n",
    "val_labels = []\n",
    "\n",
    "# Loop over the flower types and add image paths and labels to the lists\n",
    "# for i, flower_type in enumerate(flower_types):\n",
    "#     folder_path = os.path.join(data_folder, flower_type)\n",
    "#     files = os.listdir(folder_path)\n",
    "#     random.shuffle(files)\n",
    "#     split_index = int(0.8 * len(files))\n",
    "#     train_files = files[:split_index]\n",
    "#     val_files = files[split_index:]\n",
    "    \n",
    "#     for file_name in train_files:\n",
    "#         if file_name.endswith('.jpg'):\n",
    "#             image_path = os.path.join(folder_path, file_name)\n",
    "#             train_image_paths.append(image_path)\n",
    "#             train_labels.append(i)\n",
    "            \n",
    "#             os.makedirs(os.path.join(train_data_folder, flower_type), exist_ok=True)\n",
    "#             shutil.copy(image_path, os.path.join(train_data_folder, flower_type, file_name))\n",
    "            \n",
    "#     for file_name in val_files:\n",
    "#         if file_name.endswith('.jpg'):\n",
    "#             image_path = os.path.join(folder_path, file_name)\n",
    "#             val_image_paths.append(image_path)\n",
    "#             val_labels.append(i)\n",
    "            \n",
    "#             os.makedirs(os.path.join(validation_data_folder, flower_type), exist_ok=True)\n",
    "#             shutil.copy(image_path, os.path.join(validation_data_folder, flower_type, file_name))\n",
    "            \n",
    "# Define the data generators for the training and validation sets\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_folder,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    validation_data_folder,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6/58 [==>...........................] - ETA: 6:27"
     ]
    }
   ],
   "source": [
    "train_features = vgg_model.predict(train_generator,verbose=1)\n",
    "val_features = vgg_model.predict(validation_generator,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(learning_rate=0.00006, beta_1=0.9, beta_2=0.999, epsilon=None, amsgrad=False)\n",
    "model_VGG16.compile(loss=\"binary_crossentropy\", optimizer=adam, metrics=METRICS)\n",
    "history_vgg = Train_CNN_Model(50, model_VGG16, train_generator, validation_generator, export_dir=\"./export\", name=\"vgg_t1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visualize_history(history_vgg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet-50 architecture\n",
    "\"\"\"We will re-built the Resnet-50 architecture with reference from []()\"\"\"\n",
    "\n",
    "\"\"\"First block is the initial block for Layer0 consists of a size of 7*7 kernel and 0 padding,\n",
    "Since x_shortcut are 3 matrixes, and we can only add input and output if they are in the same shape.\n",
    "Therefore, if the convolution + Batch Normalization operation allows to do so, we will take use of it in the first and following block\n",
    "At the end of the block we will create 3*3 max pooling to reduce the dimension of the input.\"\"\"\n",
    "\n",
    "\n",
    "def initial_block(Input, filters, stride=1, size=7):\n",
    "    x = Conv2D(filters, kernel_size=(size, size), strides=(stride, stride), padding=\"same\")(Input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3))(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"The next block is call the convotional expand block, this block is expand from original convolutional block.\n",
    "In this block, first we create the convolution 2D with kernel size of 3x3, with 64 filter (kernels) and at last 1x1,\n",
    "with 256 kernel\n",
    "These three layers are repeated in total 3 time so giving us 9 layers in this step.\n",
    "In expand convolution bottleneck block, the usage of projection shortcut will be introduced. The projection shortcut is used to match dimensions (done by 1x1 convolutions).\n",
    "The shortcuts go across feature maps of two sizes, they are performed with a stride of 2.\"\"\"\n",
    "\n",
    "\n",
    "def conv_exp_bottleneck_block(Input, filters, k_size, k2_size, stride=1):\n",
    "    # Contracting 1*1 conv\n",
    "    x = Conv2D(filters, kernel_size=(k2_size, k2_size), strides=(stride, stride), padding=\"same\")(Input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    # Depth preserving 3*3 conv\n",
    "    x = Conv2D(filters, kernel_size=(k_size, k_size), strides=(1, 1), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    # Expanding 1*1 Conv\n",
    "    x = Conv2D(filters * 4, kernel_size=(k2_size, k2_size), strides=(1, 1), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Projection shortcut\n",
    "    skip_conv = Conv2D(filters * 4, kernel_size=(k2_size, k2_size), strides=(stride, stride), padding=\"same\")(Input)\n",
    "    skip = BatchNormalization()(skip_conv)\n",
    "\n",
    "    # Skip connection\n",
    "    x = Add()([x, skip])\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"Normal Convolutional bottleneck block will containfirst conv block of kernel of size 1x1, 128 kernels\n",
    "After that a conv block of a kernel of 3x3, with 128 kernels and\n",
    "at last we expand conv block a kernel of 1x1, with 512 kernels. In this function, we eliminate project shortcut to reduce training time \"\"\"\n",
    "\n",
    "\n",
    "def convnorm_bottleneck_block(Input, filters, k_size, k2_size, stride=1):\n",
    "    # Contracting 1*1 conv\n",
    "    x = Conv2D(filters, kernel_size=(k2_size, k2_size), strides=(stride, stride), padding=\"same\")(Input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    # Depth preserving 3*3 Conv\n",
    "    x = Conv2D(filters, kernel_size=(k_size, k_size), strides=(stride, stride), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    # Expanding 1*1 Conv\n",
    "    x = Conv2D(filters * 4, kernel_size=(k2_size, k2_size), strides=(stride, stride), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Identity skip connection\n",
    "    x = Add()([x, Input])\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"This function is responsible for building a complete resnet50 with deep bottleneck architecture.\n",
    "First input shape must be changed to match with the iterator image size of 27x27x3.\n",
    "h,w will be input height and width.\n",
    "no_of_outputs is the number of classes to learn from.\n",
    "activation type will let we choose between different activation type.\n",
    "initial value of first conv block stride is 2 and initial conv block will have kernel 7x7.\n",
    " \"\"\"\n",
    "\n",
    "\n",
    "def build_bottleneck_resnet(h, w, no_of_outputs, activation_type, first_conv_stride=2, first_conv_kernel_size=7):\n",
    "    # Creating input tensor\n",
    "    inputs = Input(shape=(h, w, 3), name=\"image_input\")\n",
    "\n",
    "    # STAGE 1:\n",
    "    # Inital Conv block like paper architecture shown -> 1 layer\n",
    "    x = initial_block(inputs, 64, first_conv_stride, first_conv_kernel_size)\n",
    "\n",
    "    # STAGE 2\n",
    "    # In the following stage, we make it little bit different, to take into account the usage of project shortcut.\n",
    "    # To make the dimension compatible first.\n",
    "    # Expanding block1 with projection shortcut. -> 3 layers\n",
    "    x = conv_exp_bottleneck_block(x, 64, 3, 1, 1)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    # Repeating block of Conv1 Stage 2 - paper value is repeat 3 times. However, we create the conv expanding block in previous step\n",
    "    # in this step, only repeat 2 times -> 6 layers\n",
    "    for i in range(2):\n",
    "        x = convnorm_bottleneck_block(x, 64, 3, 1, 1)\n",
    "        x = Activation(\"relu\")(x)\n",
    "\n",
    "    # STAGE 3\n",
    "    # Expanding block2 with projection shortcut, the reason is the same with stage 2 -> 3 layers\n",
    "    x = conv_exp_bottleneck_block(x, 128, 3, 1, 2)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    # Repeating block of Conv2. Initial value with normal resnet architecture is 4. However, since the usage of\n",
    "    # expanding convolution is used above, we reduce it to only 3 repitation. -> 9 layer\n",
    "    for i in range(3):\n",
    "        x = convnorm_bottleneck_block(x, 128, 3, 1)\n",
    "        x = Activation(\"relu\")(x)\n",
    "\n",
    "    # STAGE 4\n",
    "    # Expanding block3 with projection shortcut -> 3 layers\n",
    "    x = conv_exp_bottleneck_block(x, 256, 3, 1, 2)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    # Repeating block of Conv3 -> 15 layers\n",
    "    for i in range(5):\n",
    "        x = convnorm_bottleneck_block(x, 256, 3, 1, 1)\n",
    "        x = Activation(\"relu\")(x)\n",
    "\n",
    "    # STAGE 5\n",
    "    # Expanding block4 with projection shortcut -> 3 layers\n",
    "    x = conv_exp_bottleneck_block(x, 512, 3, 1, 2)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    # Repeating block of Conv4 ->  6 layers\n",
    "    for i in range(2):\n",
    "        x = convnorm_bottleneck_block(x, 512, 3, 1, 1)\n",
    "        x = Activation(\"relu\")(x)\n",
    "\n",
    "    shape = K.int_shape(x)\n",
    "\n",
    "    # Final Stage - 1 layers\n",
    "    # Average pooling layer\n",
    "    x = AveragePooling2D(pool_size=(shape[1], shape[2]), strides=(1, 1))(x)\n",
    "    # x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Classifier Block\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(no_of_outputs, activation=activation_type)(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet50_model = build_bottleneck_resnet(224, 224, 3, \"binary_crossentropy\", \"sigmoid\", 2, 7)\n",
    "ResNet50_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "% % time\n",
    "adam = Adam(learning_rate=0.00006, beta_1=0.9,\n",
    "            beta_2=0.999, epsilon=None, amsgrad=False)\n",
    "ResNet50_model.compile(loss='binary_crossentropy',\n",
    "                       optimizer=adam, metrics=METRICS)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history_resnet50 = Train_CNN_Model(\n",
    "    50, ResNet50_model, train_generator, validation_generator, export_dir=\"./export\", name=\"resnet50_task1_final\"\n",
    ")\n",
    "history_resnet50"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visualize_history(history_resnet50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "#\n",
    "# def recommend_similar_images(vgg_model, image_path, data_folder, flower_types, top_n=10):\n",
    "#     # Load and preprocess the input image\n",
    "#     img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
    "#     x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "#     x = np.expand_dims(x, axis=0)\n",
    "#     x = tf.keras.applications.vgg16.preprocess_input(x)\n",
    "#\n",
    "#     # Extract the feature vector of the input image\n",
    "#     features = vgg_model.predict(x)\n",
    "#\n",
    "#     # Calculate cosine similarities between the input image and all the images in the dataset\n",
    "#     similarities = []\n",
    "#     for i, flower_type in enumerate(flower_types):\n",
    "#         folder_path = os.path.join(data_folder, flower_type)\n",
    "#         for file_name in os.listdir(folder_path):\n",
    "#             if file_name.endswith('.jpg'):\n",
    "#                 image_path = os.path.join(folder_path, file_name)\n",
    "#                 img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
    "#                 x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "#                 x = np.expand_dims(x, axis=0)\n",
    "#                 x = tf.keras.applications.vgg16.preprocess_input(x)\n",
    "#                 features_i = vgg_model.predict(x)\n",
    "#                 similarity = cosine_similarity(features, features_i)\n",
    "#                 similarities.append((image_path, flower_type, similarity[0][0]))\n",
    "#\n",
    "#     # Sort the similarities in descending order and select the top_n images\n",
    "#     similarities = sorted(similarities, key=lambda x: x[2], reverse=True)[:top_n]\n",
    "#\n",
    "#     # Display the input image, filename, and flower type\n",
    "#     display(Image(filename=image_path))\n",
    "#     print(\"Input Image: \", os.path.basename(image_path), flower_types[train_labels[image_paths.index(image_path)]])\n",
    "#\n",
    "#     # Display the top_n similar images with their filenames, flower types, and cosine similarity scores\n",
    "#     for image_path, flower_type, similarity in similarities:\n",
    "#         display(Image(filename=image_path))\n",
    "#         print(os.path.basename(image_path), flower_type, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommend_similar_images(vgg_model, 'data/Flowers/Babi/babi_1.jpg', 'data/Flowers', flower_types)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
