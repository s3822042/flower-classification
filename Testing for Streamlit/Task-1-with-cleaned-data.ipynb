{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore  the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    " \n",
    "#configure\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='darkgrid',color_codes=True)\n",
    "\n",
    "#model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#preprocess.\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#dl libraraies\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# specifically for models\n",
    "from keras.layers import Flatten,Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, MaxPool2D, GlobalAveragePooling2D\n",
    "from keras.layers import GlobalAvgPool2D as GAP, Dense, Dropout\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications import ResNet50V2\n",
    "\n",
    "#callbacks\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    " \n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "# specifically for manipulating zipped images and getting numpy arrays of pixel values of images.\n",
    "import cv2                  \n",
    "import numpy as np  \n",
    "from tqdm import tqdm\n",
    "import os                   \n",
    "from random import shuffle  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We loop through all the images in all the directories, load each image using OpenCV, resize it into a fixed size (200) and append the image data and its corresponding label to the global lists IMAGE and LABEL, respectively. \n",
    "- Then, we return these lists to create the training and validation dataset for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 931/931 [00:03<00:00, 292.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 353/353 [00:05<00:00, 59.43it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 696/696 [00:02<00:00, 283.47it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 518/518 [00:01<00:00, 296.34it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 969/969 [00:03<00:00, 290.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 360/360 [00:01<00:00, 243.00it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 171/171 [00:00<00:00, 200.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 623/623 [00:02<00:00, 271.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Image: 4621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "IMAGE=[]\n",
    "LABEL=[]\n",
    "IMG_SIZE=200\n",
    "\n",
    "def assign_label(img,flower_category):\n",
    "    return flower_category\n",
    "\n",
    "def make_train_data(flower_category,DIR):\n",
    "    for img in tqdm(os.listdir(DIR)):\n",
    "        label=assign_label(img,flower_category)\n",
    "        path = os.path.join(DIR,img)\n",
    "        img = cv2.imread(path,cv2.IMREAD_COLOR)\n",
    "        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "        \n",
    "        IMAGE.append(np.array(img))\n",
    "        LABEL.append(str(label))\n",
    "\n",
    "FLOWER_DIR = []\n",
    "flower_categories = [\"Babi\", \"Calimerio\", \"Chrysanthemum\", \"Hydrangeas\", \"Lisianthus\", \"Pingpong\", \"Rosy\", \"Tana\"]\n",
    "\n",
    "for category in flower_categories:\n",
    "    flower_dir = f\"data/Flowers/{category}\"\n",
    "    FLOWER_DIR.append(flower_dir)\n",
    "    make_train_data(category, flower_dir)\n",
    "print('Total Image: ' + str(len(IMAGE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We encode the labels using LabelEncoder, convert them to one-hot encoded vectors and store them in Y. \n",
    "- Then, we convert the image data to numpy array, scale the data to values between 0-1 and store them in X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_categories = 8\n",
    "\n",
    "label_encoder=LabelEncoder()\n",
    "Y=label_encoder.fit_transform(LABEL)\n",
    "Y=to_categorical(Y,num_categories)\n",
    "X=np.array(IMAGE)\n",
    "X=X/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We split the data into two set:\n",
    "    + Training dataset: 80% of the total dataset.\n",
    "    + Validation dataset: 20% of the total dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shapes: (3696, 200, 200, 3) (3696, 8)\n",
      "Test set shapes: (925, 200, 200, 3) (925, 8)\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "\n",
    "# Confirm the shapes of the data splits\n",
    "print(\"Train set shapes:\", x_train.shape, y_train.shape)\n",
    "print(\"Test set shapes:\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We ensure that the same train-test split is generated every time we run the code, which is useful for reproducibility.\n",
    "- This allows you to compare the results of different models or hyperparameters on the same train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "rn.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By randomly applying changes to the original images, these configurations enable the development of augmented images, which are slightly changed copies of the original images. \n",
    "- This can assist to increase the size and variability of the training set, which can improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug = ImageDataGenerator(\n",
    "                featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "                samplewise_center=False,  # set each sample mean to 0\n",
    "                featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "                samplewise_std_normalization=False,  # divide each input by its std\n",
    "                zca_whitening=False,  # apply ZCA whitening\n",
    "                rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                zoom_range = 0.1, # Randomly zoom image \n",
    "                width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "                height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "                horizontal_flip=True,  # randomly flip images\n",
    "                vertical_flip=False)  # randomly flip images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We design this model with 5 convolution layers with max pooling layers in between, followed by a flatten layer, a dense layer and an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (IMG_SIZE,IMG_SIZE,3)))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "cnn_model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',activation ='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    " \n",
    "\n",
    "cnn_model.add(Conv2D(filters =96, kernel_size = (3,3),padding = 'Same',activation ='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "cnn_model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same',activation ='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "cnn_model.add(Conv2D(filters = 256, kernel_size = (3,3),padding = 'Same',activation ='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(512))\n",
    "cnn_model.add(Activation('relu'))\n",
    "cnn_model.add(Dense(num_categories, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We set up the optimizer, loss function and evaluation metric before training the model.\n",
    "- Then, we print a summary of the model architecture which includes the number of parameters in each layer and the output shape of each layer as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 200, 200, 32)      2432      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 100, 100, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 100, 100, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 50, 50, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 50, 50, 96)        55392     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 25, 25, 96)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 25, 25, 128)       110720    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 12, 12, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 6, 6, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               4719104   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 4104      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,205,416\n",
      "Trainable params: 5,205,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The batch size and total number of epochs are assigned to the model. \n",
    "-\n",
    "- Then, we fit the training data into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "57/57 [==============================] - 290s 5s/step - loss: 1.9816 - accuracy: 0.2037 - val_loss: 1.9294 - val_accuracy: 0.2454\n",
      "Epoch 2/70\n",
      "57/57 [==============================] - ETA: 0s - loss: 1.9074 - accuracy: 0.2481"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20580\\1492912142.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_lr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.000001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m cnn_history = cnn_model.fit(data_aug.flow(x_train,y_train, batch_size= batch_size),\n\u001b[0m\u001b[0;32m      7\u001b[0m                               \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                               verbose = 1, steps_per_epoch=x_train.shape[0] // batch_size)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1727\u001b[0m                             \u001b[0msteps_per_execution\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1728\u001b[0m                         )\n\u001b[1;32m-> 1729\u001b[1;33m                     val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m   1730\u001b[0m                         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m                         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   2070\u001b[0m                         ):\n\u001b[0;32m   2071\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2072\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2073\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2074\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 894\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    895\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    141\u001b[0m       (concrete_function,\n\u001b[0;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[1;33m     return concrete_function._call_flat(\n\u001b[0m\u001b[0;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1756\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1757\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 70\n",
    "\n",
    "callbacks = [EarlyStopping(patience=5, restore_best_weights=True), ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.3, min_lr=0.000001)]\n",
    "\n",
    "cnn_history = cnn_model.fit(data_aug.flow(x_train,y_train, batch_size= batch_size),\n",
    "                              epochs = epochs, validation_data = (x_test,y_test),\n",
    "                              verbose = 1, steps_per_epoch=x_train.shape[0] // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cnn_history.history['loss'])\n",
    "plt.plot(cnn_history.history['val_loss'])\n",
    "plt.title('CNN Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cnn_history.history['accuracy'])\n",
    "plt.plot(cnn_history.history['val_accuracy'])\n",
    "plt.title('CNN Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50V2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet50V2_base_model = ResNet50V2(input_shape=(IMG_SIZE,IMG_SIZE,3), include_top=False)\n",
    "\n",
    "for layer in ResNet50V2_base_model.layers[:170]:\n",
    "    layer.trainable = False\n",
    "\n",
    "ResNet50V2_model = Sequential([\n",
    "    ResNet50V2_base_model,\n",
    "    GAP(),\n",
    "    Dense(512, activation='relu', kernel_initializer='he_normal'),\n",
    "    Dropout(0.4),\n",
    "    Dense(num_categories, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet50V2_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "\n",
    "ResNet50V2_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet50V2_history = ResNet50V2_model.fit(data_aug.flow(x_train,y_train, batch_size= batch_size),\n",
    "                              epochs = epochs, validation_data = (x_test,y_test),\n",
    "                              verbose = 1, steps_per_epoch=x_train.shape[0] // batch_size, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ResNet50V2_history.history['loss'])\n",
    "plt.plot(ResNet50V2_history.history['val_loss'])\n",
    "plt.title('ResNet50V2 Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ResNet50V2_history.history['accuracy'])\n",
    "plt.plot(ResNet50V2_history.history['val_accuracy'])\n",
    "plt.title('ResNet50V2 Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG19 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This model is built on top of the pre-trained VGG19 model from Keras.\n",
    "- We firstly freeze the weights of the first 19 layers of the pre-trained VGG19 to simplify the model.\n",
    "- Then, we create our model by adding more layers to the model: \n",
    "    + A MaxPool2D layer to downsample the feature maps.\n",
    "    + A Flatten layer to convert output into a 1D array.\n",
    "    + A Dense layer with a softmax activation function to classify the images into one of the eight categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG19_based_model = VGG19(input_shape=(IMG_SIZE,IMG_SIZE,3), include_top=False)\n",
    "\n",
    "for layer in VGG19_based_model.layers[:19]:\n",
    "    layer.trainable = False\n",
    "\n",
    "VGG19_model = Sequential([\n",
    "    VGG19_based_model,\n",
    "    MaxPool2D((2,2) , strides = 2),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu', kernel_initializer='he_normal'),\n",
    "    Dense(num_categories , activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We set up the optimizer, loss function and evaluation metric before training the model.\n",
    "- Additionally, we set up a callback function to reduce the learning rate when the validation accuracy does not improve after 2 epochs. This helps the model to converge better and avoid overfitting.\n",
    "- Then, we print a summary of the model architecture which includes the number of parameters in each layer and the output shape of each layer as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG19_model.compile(optimizer =Adam(lr=0.001), loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "\n",
    "VGG19_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The batch size and total number of epochs are assigned to the model. \n",
    "- Then, we fit the training data into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG19_history = VGG19_model.fit(data_aug.flow(x_train,y_train, batch_size= batch_size),\n",
    "                              epochs = epochs, validation_data = (x_test,y_test),\n",
    "                              verbose = 1, steps_per_epoch=x_train.shape[0] // batch_size, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(VGG19_history.history['loss'])\n",
    "plt.plot(VGG19_history.history['val_loss'])\n",
    "plt.title('vgg19 Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(VGG19_history.history['accuracy'])\n",
    "plt.plot(VGG19_history.history['val_accuracy'])\n",
    "plt.title('VGG19 Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=VGG19_model.predict(x_test)\n",
    "pred_digits=np.argmax(pred,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Classified Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "cor_class=[]\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if(np.argmax(y_test[i])==pred_digits[i]):\n",
    "        cor_class.append(i)\n",
    "    if(len(cor_class)==9):\n",
    "        break\n",
    "\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "count=0\n",
    "fig,ax=plt.subplots(3,3)\n",
    "fig.set_size_inches(15,15)\n",
    "for i in range (4):\n",
    "    for j in range (2):\n",
    "        ax[i,j].imshow(x_test[cor_class[count]])\n",
    "        ax[i,j].set_title(\"Predicted Flower : \"+str(label_encoder.inverse_transform([pred_digits[cor_class[count]]]))+\"\\n\"+\"Actual Flower : \"+str(label_encoder.inverse_transform(np.argmax(y_test[cor_class[count]],axis=0))))\n",
    "        plt.tight_layout()\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missclassified Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "mis_class=[]\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if(not np.argmax(y_test[i])==pred_digits[i]):\n",
    "        mis_class.append(i)\n",
    "    if(len(mis_class)==9):\n",
    "        break\n",
    "\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "count=0\n",
    "fig,ax=plt.subplots(3,3)\n",
    "fig.set_size_inches(15,15)\n",
    "for i in range (4):\n",
    "    for j in range (2):\n",
    "        ax[i,j].imshow(x_test[mis_class[count]])\n",
    "        ax[i,j].set_title(\"Predicted Flower : \"+str(label_encoder.inverse_transform([pred_digits[mis_class[count]]]))+\"\\n\"+\"Actual Flower : \"+str(label_encoder.inverse_transform(np.argmax(y_test[mis_class[count]]))))\n",
    "        plt.tight_layout()\n",
    "        count+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
